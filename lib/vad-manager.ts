/**
 * Smart VAD Manager - æ™ºèƒ½è¯­éŸ³æ´»åŠ¨æ£€æµ‹ç®¡ç†å™¨
 *
 * è§£å†³ 3 å¤§è¯­éŸ³äº¤äº’é—®é¢˜ï¼š
 * 1. AI è¿‡æ—©æ‰“æ–­ç”¨æˆ·ï¼ˆç”¨æˆ·åœé¡¿æ€è€ƒæ—¶ï¼‰
 * 2. AI å»¶è¿Ÿå“åº”ï¼ˆç”¨æˆ·è¯´å®Œåï¼‰
 * 3. ç”¨æˆ·æ— æ³•æ‰“æ–­ AI
 *
 * åŸºäº Silero VAD (@ricky0123/vad-web)
 */

import { MicVAD } from "@ricky0123/vad-web"

export type ConversationContext = 'emotional' | 'simple' | 'normal'

export interface VADConfig {
  /** ç”¨æˆ·å¼€å§‹è¯´è¯æ—¶çš„å›è°ƒ */
  onSpeechStart: () => void

  /** ç”¨æˆ·åœæ­¢è¯´è¯æ—¶çš„å›è°ƒï¼ˆæä¾›éŸ³é¢‘æ•°æ®ï¼‰ */
  onSpeechEnd: (audio: Float32Array) => void

  /** å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆå†³å®šåœé¡¿é˜ˆå€¼ï¼‰ */
  context?: ConversationContext
}

/**
 * æ™ºèƒ½ VAD ç®¡ç†å™¨
 */
export class SmartVADManager {
  private vad: MicVAD | null = null
  private currentContext: ConversationContext = 'normal'

  /**
   * åœé¡¿é˜ˆå€¼é…ç½®ï¼ˆå•ä½ï¼šå¸§æ•°ï¼Œ1 å¸§ â‰ˆ 100msï¼‰
   *
   * emotional: 2 ç§’ - ç”¨äºæƒ…ç»ªå€¾è¯‰åœºæ™¯ï¼ˆç»™ç”¨æˆ·å……åˆ†æ—¶é—´æ•´ç†æ€ç»ªï¼‰
   * simple: 0.8 ç§’ - ç”¨äºç®€å•é—®ç­”åœºæ™¯ï¼ˆå¿«é€Ÿå“åº”ï¼‰
   * normal: 1.2 ç§’ - é»˜è®¤åœºæ™¯ï¼ˆå¹³è¡¡ï¼‰
   */
  private pauseThresholds: Record<ConversationContext, number> = {
    emotional: 20,  // 2 ç§’
    simple: 8,      // 0.8 ç§’
    normal: 12      // 1.2 ç§’
  }

  /**
   * åˆå§‹åŒ– VAD
   */
  async initialize(config: VADConfig): Promise<void> {
    this.currentContext = config.context || 'normal'

    console.log("[Smart VAD] Initializing with context:", this.currentContext)
    console.log("[Smart VAD] Pause threshold:", this.pauseThresholds[this.currentContext] * 0.1, "seconds")

    try {
      this.vad = await MicVAD.new({
        // è¯­éŸ³æ£€æµ‹é˜ˆå€¼ï¼ˆ0-1ï¼‰
        // è¶Šé«˜ = è¶Šä¿å®ˆï¼ˆå¯èƒ½æ¼æ‰è½»å£°ï¼‰
        // è¶Šä½ = è¶Šæ•æ„Ÿï¼ˆå¯èƒ½è¯¯åˆ¤å™ªéŸ³ï¼‰
        positiveSpeechThreshold: this.getSpeechThreshold(),

        // é™éŸ³æ£€æµ‹é˜ˆå€¼ï¼ˆ0-1ï¼‰
        negativeSpeechThreshold: 0.35,

        // å…³é”®å‚æ•°ï¼šè¿ç»­å¤šå°‘å¸§é™éŸ³æ‰ç®—"è¯´å®Œ"
        redemptionFrames: this.pauseThresholds[this.currentContext],

        // æœ€å°‘è¯­éŸ³å¸§æ•°ï¼ˆé¿å…è¯¯è§¦å‘ï¼‰
        minSpeechFrames: this.currentContext === 'emotional' ? 2 : 3,

        // è¯­éŸ³å‰å¡«å……å¸§ï¼ˆé¿å…å¼€å¤´è¢«æˆªæ–­ï¼‰
        preSpeechPadFrames: this.currentContext === 'emotional' ? 2 : 1,

        // å›è°ƒå‡½æ•°
        onSpeechStart: () => {
          console.log("[Smart VAD] ğŸ¤ User started speaking")
          config.onSpeechStart()
        },

        onSpeechEnd: (audio) => {
          const durationMs = (audio.length / 16000) * 1000
          console.log("[Smart VAD] ğŸ›‘ User stopped speaking, duration:", durationMs.toFixed(0), "ms")
          config.onSpeechEnd(audio)
        },

        onVADMisfire: () => {
          console.log("[Smart VAD] âš ï¸ False alarm - not speech (background noise)")
        }
      })

      this.vad.start()
      console.log("[Smart VAD] âœ… Started listening")

    } catch (error) {
      console.error("[Smart VAD] âŒ Failed to initialize:", error)
      throw error
    }
  }

  /**
   * æ ¹æ®åœºæ™¯è·å–è¯­éŸ³æ£€æµ‹é˜ˆå€¼
   */
  private getSpeechThreshold(): number {
    switch (this.currentContext) {
      case 'emotional':
        return 0.4  // æ›´æ•æ„Ÿï¼ˆæ•æ‰å“½å’½ã€è½»å£°ï¼‰
      case 'simple':
        return 0.5  // æ ‡å‡†
      case 'normal':
      default:
        return 0.5  // æ ‡å‡†
    }
  }

  /**
   * æ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡
   *
   * æ³¨æ„ï¼šå½“å‰ @ricky0123/vad ä¸æ”¯æŒåŠ¨æ€æ›´æ–°å‚æ•°
   * éœ€è¦åœ¨ä¸‹æ¬¡åˆå§‹åŒ–æ—¶ç”Ÿæ•ˆ
   */
  updateContext(context: ConversationContext): void {
    if (context !== this.currentContext) {
      console.log("[Smart VAD] Context will change to:", context, "(on next init)")
      this.currentContext = context
    }
  }

  /**
   * é”€æ¯ VADï¼ˆé‡Šæ”¾éº¦å…‹é£èµ„æºï¼‰
   */
  destroy(): void {
    if (this.vad) {
      this.vad.destroy()
      this.vad = null
      console.log("[Smart VAD] ğŸ”´ Stopped listening")
    }
  }

  /**
   * æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿è¡Œ
   */
  isRunning(): boolean {
    return this.vad !== null
  }
}

/**
 * æ£€æµ‹å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´åœé¡¿é˜ˆå€¼ï¼‰
 */
export function detectConversationContext(
  lastAIMessage: string,
  userResponse: string,
  conversationHistory: { role: string; content: string }[] = []
): ConversationContext {
  // 1. æ£€æµ‹ AI æ˜¯å¦é—®äº†å¼€æ”¾å¼é—®é¢˜
  const openEndedKeywords = [
    'æ€ä¹ˆæ ·', 'æ„Ÿè§‰', 'æƒ³æ³•', 'ä¸ºä»€ä¹ˆ', 'è¯¦ç»†', 'è¯´è¯´', 'æè¿°',
    'how', 'feel', 'think', 'why', 'tell me', 'describe', 'what happened'
  ]
  const isOpenEnded = openEndedKeywords.some(kw =>
    lastAIMessage.toLowerCase().includes(kw.toLowerCase())
  )

  // 2. æ£€æµ‹ AI æ˜¯å¦é—®äº†ç®€å•é—®é¢˜
  const simpleKeywords = [
    'æ˜¯å—', 'å¯¹å—', 'å¥½å—', 'è¦ä¸è¦', 'å¯ä»¥å—', 'æ„¿æ„å—',
    'yes or no', 'right', 'okay', 'do you', 'would you', 'can you'
  ]
  const isSimple = simpleKeywords.some(kw =>
    lastAIMessage.toLowerCase().includes(kw.toLowerCase())
  )

  // 3. æ£€æµ‹ç”¨æˆ·å›å¤é•¿åº¦
  const isShortResponse = userResponse.length < 20

  // 4. æ£€æµ‹æƒ…ç»ªåŒ–å†…å®¹ï¼ˆç”¨æˆ·å¯èƒ½åœ¨å€¾è¯‰ï¼‰
  const emotionalKeywords = [
    'ç„¦è™‘', 'æŠ‘éƒ', 'éš¾è¿‡', 'ç—›è‹¦', 'å´©æºƒ', 'å‹åŠ›', 'å®³æ€•', 'æ‹…å¿ƒ',
    'anxious', 'depressed', 'sad', 'pain', 'stress', 'scared', 'worried'
  ]
  const hasEmotionalContent = emotionalKeywords.some(kw =>
    userResponse.toLowerCase().includes(kw.toLowerCase())
  )

  // 5. æ£€æµ‹å¯¹è¯å†å²ï¼ˆæ˜¯å¦å¤„äºæ·±åº¦å’¨è¯¢é˜¶æ®µï¼‰
  const isDeepSession = conversationHistory.length > 10

  // å†³ç­–é€»è¾‘
  if ((isOpenEnded || hasEmotionalContent) && !isShortResponse) {
    return 'emotional'  // éœ€è¦æ›´å¤šæ—¶é—´æ€è€ƒå’Œå€¾è¯‰
  }

  if (isSimple || isShortResponse) {
    return 'simple'  // å¿«é€Ÿå“åº”
  }

  if (isDeepSession && hasEmotionalContent) {
    return 'emotional'  // æ·±åº¦å’¨è¯¢é˜¶æ®µ
  }

  return 'normal'  // é»˜è®¤
}

/**
 * éŸ³é¢‘æ•°æ®è½¬ WAV Blobï¼ˆç”¨äºå‘é€åˆ° STT APIï¼‰
 */
export function audioBufferToWav(audioData: Float32Array, sampleRate: number = 16000): Blob {
  const numChannels = 1
  const bitsPerSample = 16
  const bytesPerSample = bitsPerSample / 8
  const blockAlign = numChannels * bytesPerSample

  const dataLength = audioData.length * bytesPerSample
  const buffer = new ArrayBuffer(44 + dataLength)
  const view = new DataView(buffer)

  // WAV æ–‡ä»¶å¤´
  const writeString = (offset: number, string: string) => {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i))
    }
  }

  writeString(0, 'RIFF')
  view.setUint32(4, 36 + dataLength, true)
  writeString(8, 'WAVE')
  writeString(12, 'fmt ')
  view.setUint32(16, 16, true)
  view.setUint16(20, 1, true)
  view.setUint16(22, numChannels, true)
  view.setUint32(24, sampleRate, true)
  view.setUint32(28, sampleRate * blockAlign, true)
  view.setUint16(32, blockAlign, true)
  view.setUint16(34, bitsPerSample, true)
  writeString(36, 'data')
  view.setUint32(40, dataLength, true)

  // éŸ³é¢‘æ•°æ®ï¼ˆFloat32 â†’ Int16ï¼‰
  const volume = 0.8
  let offset = 44
  for (let i = 0; i < audioData.length; i++) {
    const sample = Math.max(-1, Math.min(1, audioData[i]))
    view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true)
    offset += 2
  }

  return new Blob([view], { type: 'audio/wav' })
}
