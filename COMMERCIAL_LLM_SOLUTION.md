# ğŸš€ AI å¿ƒç†å’¨è¯¢å¸ˆ - çœŸæ­£å¯å•†ç”¨çš„å…è´¹æ–¹æ¡ˆ

## âŒ GLM-4-Flash çš„é—®é¢˜

æ‚¨è¯´å¾—å¯¹ï¼**100ä¸‡ tokens/å¤© â‰ˆ çº¦ 2000 æ¬¡å¯¹è¯**ï¼Œæ— æ³•å•†ç”¨ã€‚

å‡è®¾æ¯æ¬¡å¯¹è¯ 500 tokensï¼š
- 100 ç”¨æˆ·/å¤© Ã— 20 æ¬¡å¯¹è¯ = 2000 æ¬¡ âœ… åˆšå¥½å¤Ÿ
- 500 ç”¨æˆ·/å¤© Ã— 20 æ¬¡å¯¹è¯ = 10000 æ¬¡ âŒ è¿œè¶…å…è´¹é¢åº¦

**ç»“è®º**ï¼šGLM-4-Flash åªé€‚åˆå°è§„æ¨¡æµ‹è¯•ï¼Œä¸é€‚åˆå•†ç”¨ã€‚

---

## âœ… çœŸæ­£å¯å•†ç”¨çš„ 3 ç§æ–¹æ¡ˆ

### æ–¹æ¡ˆ Aï¼šGroq APIï¼ˆæ¨è - è¶…å¿«é€Ÿåº¦ï¼‰

**å…è´¹é¢åº¦**ï¼š
- **30,000 tokens/åˆ†é’Ÿ**ï¼ˆTPMï¼‰
- **30 requests/åˆ†é’Ÿ**ï¼ˆRPMï¼‰
- **æ°¸ä¹…å…è´¹**ï¼ˆç›®å‰ï¼‰

**è®¡ç®—**ï¼š
- 30,000 TPM Ã— 60 åˆ†é’Ÿ Ã— 24 å°æ—¶ = **43,200,000 tokens/å¤©**
- å‡è®¾æ¯æ¬¡å¯¹è¯ 500 tokens = **86,400 æ¬¡å¯¹è¯/å¤©**

**å•†ç”¨èƒ½åŠ›**ï¼š
- âœ… 1000 ç”¨æˆ·/å¤© Ã— 86 æ¬¡å¯¹è¯ = 86,000 æ¬¡ âœ…
- âœ… é€Ÿåº¦æå¿«ï¼ˆ300+ tokens/ç§’ï¼‰
- âœ… ä½¿ç”¨ Llama 3.3 70Bï¼ˆæ¥è¿‘ GPT-4ï¼‰

**æˆæœ¬**ï¼š
- å…è´¹é¢åº¦å†…ï¼š$0
- è¶…å‡ºåï¼š$0.59/M tokensï¼ˆè¾“å…¥ï¼‰$0.79/M tokensï¼ˆè¾“å‡ºï¼‰

**API**ï¼š
```typescript
const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
  method: "POST",
  headers: {
    "Authorization": `Bearer ${process.env.GROQ_API_KEY}`,
    "Content-Type": "application/json"
  },
  body: JSON.stringify({
    model: "llama-3.3-70b-versatile",
    messages: conversationHistory,
    temperature: 0.7,
    max_tokens: 500
  })
})
```

---

### æ–¹æ¡ˆ Bï¼šGoogle Gemini 2.0 Flashï¼ˆæœ€å¤§å…è´¹é¢åº¦ï¼‰

**å…è´¹é¢åº¦**ï¼š
- **1,000,000 tokens/åˆ†é’Ÿ**ï¼ˆRPMï¼‰
- **15 requests/åˆ†é’Ÿ**ï¼ˆRPMï¼‰
- **1,500 requests/å¤©**ï¼ˆRPDï¼‰

**è®¡ç®—**ï¼š
- 1,500 requests/å¤© Ã— 2000 tokens/request = **3,000,000 tokens/å¤©**
- å‡è®¾æ¯æ¬¡å¯¹è¯ 500 tokens = **6,000 æ¬¡å¯¹è¯/å¤©**

**å•†ç”¨èƒ½åŠ›**ï¼š
- âœ… 500 ç”¨æˆ·/å¤© Ã— 12 æ¬¡å¯¹è¯ = 6,000 æ¬¡ âœ…
- âœ… å¤šæ¨¡æ€æ”¯æŒï¼ˆå›¾ç‰‡ã€è§†é¢‘ï¼‰
- âœ… é•¿ä¸Šä¸‹æ–‡ï¼ˆ128k tokensï¼‰

**æˆæœ¬**ï¼š
- å…è´¹é¢åº¦å†…ï¼š$0
- è¶…å‡ºåï¼š$0.075/M tokensï¼ˆè¾“å…¥ï¼‰$0.30/M tokensï¼ˆè¾“å‡ºï¼‰

**API**ï¼š
```typescript
import { GoogleGenerativeAI } from "@google/generative-ai"

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY)
const model = genAI.getGenerativeModel({ model: "gemini-2.0-flash" })

const result = await model.generateContent({
  contents: [{ role: "user", parts: [{ text: userMessage }] }]
})
```

---

### æ–¹æ¡ˆ Cï¼šè‡ªæ‰˜ç®¡ Ollama + Llama 3.3ï¼ˆå®Œå…¨å…è´¹ï¼Œæ— é™åˆ¶ï¼‰

**å…è´¹é¢åº¦**ï¼š
- **æ— é™ tokens**
- **æ— é™ requests**
- **å®Œå…¨æœ¬åœ°è¿è¡Œ**

**ç¡¬ä»¶è¦æ±‚**ï¼š
- **æœ€ä½**ï¼š8GB RAM + CPUï¼ˆLlama 3.2 3Bï¼‰
- **æ¨è**ï¼š16GB RAM + GPUï¼ˆLlama 3.3 8Bï¼‰
- **æœ€ä½³**ï¼š32GB RAM + GPUï¼ˆLlama 3.3 70Bï¼‰

**ä¼˜åŠ¿**ï¼š
- âœ… å®Œå…¨å…è´¹ï¼Œæ— ä»»ä½•é™åˆ¶
- âœ… æ•°æ®å®Œå…¨ç§å¯†
- âœ… å¯å®šåˆ¶åŒ– fine-tune
- âœ… æ— ç½‘ç»œå»¶è¿Ÿ

**éƒ¨ç½²**ï¼š
```bash
# å®‰è£… Ollama
curl -fsSL https://ollama.com/install.sh | sh

# ä¸‹è½½æ¨¡å‹
ollama pull llama3.3:8b

# å¯åŠ¨æœåŠ¡
ollama serve

# API è°ƒç”¨
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.3:8b",
  "messages": [
    {"role": "user", "content": "æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘"}
  ]
}'
```

**ä½¿ç”¨ Vercel + å¤–éƒ¨ Ollama æœåŠ¡å™¨**ï¼š
1. åœ¨ç‹¬ç«‹æœåŠ¡å™¨ä¸Šè¿è¡Œ Ollama
2. æš´éœ² API ç«¯ç‚¹ï¼ˆä½¿ç”¨ ngrok æˆ– Cloudflare Tunnelï¼‰
3. Vercel è°ƒç”¨å¤–éƒ¨ Ollama API

---

## ğŸ“Š ä¸‰ç§æ–¹æ¡ˆå¯¹æ¯”

| æŒ‡æ ‡ | Groq | Gemini 2.0 Flash | Ollama (è‡ªæ‰˜ç®¡) |
|------|------|------------------|-----------------|
| **å…è´¹é¢åº¦** | 43M tokens/å¤© | 3M tokens/å¤© | æ— é™ |
| **å•†ç”¨èƒ½åŠ›** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **é€Ÿåº¦** | æå¿«ï¼ˆ300 TPSï¼‰ | å¿« | å–å†³äºç¡¬ä»¶ |
| **è´¨é‡** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| **æˆæœ¬** | $0ï¼ˆå…è´¹é¢åº¦å†…ï¼‰ | $0ï¼ˆå…è´¹é¢åº¦å†…ï¼‰ | ç¡¬ä»¶æˆæœ¬ |
| **æ•°æ®éšç§** | å‘é€åˆ° Groq | å‘é€åˆ° Google | å®Œå…¨æœ¬åœ° |
| **éƒ¨ç½²éš¾åº¦** | ç®€å• | ç®€å• | ä¸­ç­‰ |

---

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### å°å‹åº”ç”¨ï¼ˆ< 500 ç”¨æˆ·/å¤©ï¼‰
âœ… **Groq API**
- å…è´¹é¢åº¦è¶³å¤Ÿ
- é€Ÿåº¦æå¿«
- è´¨é‡æ¥è¿‘ GPT-4

### ä¸­å‹åº”ç”¨ï¼ˆ500-2000 ç”¨æˆ·/å¤©ï¼‰
âœ… **Gemini 2.0 Flash**
- å…è´¹é¢åº¦è¾ƒå¤§
- å¤šæ¨¡æ€æ”¯æŒ
- ä¾¿å®œçš„ä»˜è´¹é€‰é¡¹

### å¤§å‹åº”ç”¨ï¼ˆ> 2000 ç”¨æˆ·/å¤©ï¼‰æˆ–éœ€è¦éšç§
âœ… **Ollama è‡ªæ‰˜ç®¡**
- æ— é™åˆ¶
- å®Œå…¨å…è´¹
- æ•°æ®ç§å¯†

### æ··åˆæ–¹æ¡ˆï¼ˆæœ€ä½³ï¼‰
âœ… **Groqï¼ˆä¸»ï¼‰ + Ollamaï¼ˆå¤‡ç”¨ï¼‰**
- æ—¥å¸¸ä½¿ç”¨ Groq å…è´¹é¢åº¦
- è¶…å‡ºåè‡ªåŠ¨åˆ‡æ¢åˆ° Ollama
- é«˜å³°æœŸä½¿ç”¨ Ollama

---

## ğŸ’» å®æ–½ç¤ºä¾‹ï¼ˆæ··åˆæ–¹æ¡ˆï¼‰

```typescript
// app/api/smart-chat/route.ts
import { NextRequest, NextResponse } from "next/server"

// ä¼˜å…ˆçº§é˜Ÿåˆ—
const LLM_PROVIDERS = [
  {
    name: "groq",
    maxTokensPerDay: 43_000_000,
    usedTokensToday: 0,  // ä» Redis/DB è¯»å–
    call: async (messages) => {
      const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${process.env.GROQ_API_KEY}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model: "llama-3.3-70b-versatile",
          messages,
          temperature: 0.7,
          max_tokens: 500
        })
      })
      return response.json()
    }
  },
  {
    name: "gemini",
    maxTokensPerDay: 3_000_000,
    usedTokensToday: 0,
    call: async (messages) => {
      // Gemini API è°ƒç”¨
    }
  },
  {
    name: "ollama",
    maxTokensPerDay: Infinity,  // æ— é™
    usedTokensToday: 0,
    call: async (messages) => {
      const response = await fetch("http://your-ollama-server.com:11434/api/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          model: "llama3.3:8b",
          messages
        })
      })
      return response.json()
    }
  }
]

export async function POST(request: NextRequest) {
  const { messages } = await request.json()

  // æ™ºèƒ½é€‰æ‹©å¯ç”¨çš„ LLM
  for (const provider of LLM_PROVIDERS) {
    if (provider.usedTokensToday < provider.maxTokensPerDay) {
      try {
        const result = await provider.call(messages)

        // æ›´æ–°ä½¿ç”¨é‡ï¼ˆä¿å­˜åˆ° Redis/DBï¼‰
        const tokensUsed = result.usage?.total_tokens || 500
        provider.usedTokensToday += tokensUsed

        console.log(`[Smart Chat] Used ${provider.name}, tokens: ${tokensUsed}`)

        return NextResponse.json(result)
      } catch (error) {
        console.error(`[Smart Chat] ${provider.name} failed:`, error)
        // ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª provider
        continue
      }
    }
  }

  return NextResponse.json({ error: "All LLM providers exhausted" }, { status: 503 })
}
```

---

## ğŸ”„ æ¯æ—¥é‡ç½®ä½¿ç”¨é‡

```typescript
// lib/llm-usage-tracker.ts
import { Redis } from "@upstash/redis"

const redis = new Redis({
  url: process.env.UPSTASH_REDIS_URL!,
  token: process.env.UPSTASH_REDIS_TOKEN!
})

export async function getUsageToday(provider: string): Promise<number> {
  const today = new Date().toISOString().split('T')[0]
  const key = `llm_usage:${provider}:${today}`
  const usage = await redis.get(key)
  return Number(usage) || 0
}

export async function incrementUsage(provider: string, tokens: number) {
  const today = new Date().toISOString().split('T')[0]
  const key = `llm_usage:${provider}:${today}`

  await redis.incrby(key, tokens)
  await redis.expire(key, 86400)  // 24å°æ—¶åè‡ªåŠ¨åˆ é™¤
}
```

---

## ğŸ“ˆ æˆæœ¬é¢„ä¼°ï¼ˆ1000 ç”¨æˆ·/å¤©ï¼Œæ¯äºº 20 æ¬¡å¯¹è¯ï¼‰

| æ–¹æ¡ˆ | æ¯æ—¥å¯¹è¯æ•° | Tokens/å¤© | æœˆæˆæœ¬ |
|------|-----------|-----------|--------|
| **Groq** | 20,000 | 10M | $0ï¼ˆå…è´¹é¢åº¦å†…ï¼‰ |
| **Gemini** | 20,000 | 10M | $0ï¼ˆéœ€è¦è¡¥å…… Ollamaï¼‰ |
| **Ollama** | æ— é™ | æ— é™ | $0ï¼ˆç¡¬ä»¶æˆæœ¬ ~$50/æœˆ VPSï¼‰ |
| **æ··åˆ** | æ— é™ | æ— é™ | $0-50/æœˆ |

---

## âœ… æœ€ç»ˆæ¨è

**æœ€ä½³å•†ç”¨æ–¹æ¡ˆ**ï¼š**Groqï¼ˆä¸»ï¼‰ + Ollamaï¼ˆå¤‡ç”¨ï¼‰**

1. **æ—¥å¸¸ä½¿ç”¨ Groq**ï¼š
   - é€Ÿåº¦å¿«ï¼ˆ300 TPSï¼‰
   - å…è´¹é¢åº¦å¤§ï¼ˆ43M tokens/å¤©ï¼‰
   - è´¨é‡é«˜ï¼ˆLlama 3.3 70Bï¼‰

2. **é«˜å³°æœŸ/è¶…é¢åä½¿ç”¨ Ollama**ï¼š
   - å®Œå…¨å…è´¹
   - æ— é™åˆ¶
   - æ•°æ®ç§å¯†

3. **æˆæœ¬**ï¼š
   - å°å‹åº”ç”¨ï¼š$0/æœˆ
   - ä¸­å‹åº”ç”¨ï¼š$0-50/æœˆï¼ˆOllama VPSï¼‰
   - å¤§å‹åº”ç”¨ï¼š$50-200/æœˆï¼ˆæ›´å¼ºçš„ Ollama æœåŠ¡å™¨ï¼‰

---

## ğŸš€ ä¸‹ä¸€æ­¥

1. **ç«‹å³æ³¨å†Œ Groq API**ï¼šhttps://console.groq.com
2. **å¯é€‰ï¼šéƒ¨ç½² Ollama**ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
3. **å®æ–½æ™ºèƒ½ LLM è·¯ç”±**ï¼ˆè‡ªåŠ¨åˆ‡æ¢ï¼‰

**è¿™æ ·æ‚¨å°±æœ‰äº†ä¸€ä¸ªçœŸæ­£å¯å•†ç”¨ã€æˆæœ¬æä½çš„æ–¹æ¡ˆï¼** ğŸ‰
